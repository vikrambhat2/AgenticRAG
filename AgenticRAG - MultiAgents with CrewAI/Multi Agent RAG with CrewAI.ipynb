{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Agentic RAG for PDF-based Question Answering**\n",
    "\n",
    "## **Introduction**\n",
    "This notebook implements an **Agentic RAG (Retrieval-Augmented Generation) system** for querying PDFs using **CrewAI**. It employs two autonomous agents:  \n",
    "\n",
    "- **PDFRetrievalAgent**: Retrieves the most relevant document excerpts using FAISS and sentence-transformer embeddings.  \n",
    "- **LLMAgent**: Processes retrieved content and generates responses while ensuring answers are strictly grounded in the document.  \n",
    "\n",
    "The system follows a **multi-step workflow** where agents collaborate to ensure accuracy, reducing hallucination risks. This approach is useful for **research papers, legal documents, reports, or any structured knowledge retrieval task**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import Required Libraries**  \n",
    "\n",
    "This section imports the essential libraries for building an **Agentic RAG system** to process PDFs and answer user queries.  \n",
    "\n",
    "### **Libraries Overview:**  \n",
    "- **CrewAI Components (`Agent`, `Task`, `Crew`, `Process`)**  \n",
    "  - Define and manage autonomous agents for retrieval and response generation.  \n",
    "\n",
    "- **LangChain for Document Processing**  \n",
    "  - `PyPDFLoader`: Loads and extracts text from PDFs.  \n",
    "  - `RecursiveCharacterTextSplitter`: Splits documents into smaller chunks for better retrieval.  \n",
    "\n",
    "- **Vector Database for Efficient Search**  \n",
    "  - `FAISS`: A fast vector search library for storing and retrieving document embeddings.  \n",
    "  - `HuggingFaceEmbeddings`: Generates embeddings using a **pre-trained transformer model** for similarity search.  \n",
    "\n",
    "- **LLM (Language Model Processing)**  \n",
    "  - Used to generate responses strictly based on retrieved document content.  \n",
    "\n",
    "This setup ensures an efficient **retrieval-augmented workflow** where documents are processed, stored, and queried seamlessly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from crewai import LLM\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. PDF Processing and Vector Store Creation**  \n",
    "\n",
    "This function processes a PDF document, splits it into smaller chunks, and creates a vector store for efficient similarity-based search.  \n",
    "\n",
    "### **Steps:**\n",
    "1. **Load the PDF**:  \n",
    "   - The PDF document is loaded from the specified file path using a PDF loader.\n",
    "\n",
    "2. **Split the Document**:  \n",
    "   - The document is split into smaller, manageable chunks using a text splitter. The chunk size is defined to ensure context is preserved across chunks with some overlap.\n",
    "\n",
    "3. **Generate Embeddings**:  \n",
    "   - Each document chunk is converted into numerical representations (embeddings) using a pre-trained HuggingFace model. These embeddings capture the semantic meaning of the content.\n",
    "\n",
    "4. **Create FAISS Vector Store**:  \n",
    "   - The generated embeddings are stored in a FAISS vector store, which enables fast similarity search for document retrieval.\n",
    "\n",
    "5. **Return the Vector Store**:  \n",
    "   - The vector store is returned, ready for use in the query process to retrieve relevant document chunks based on similarity.\n",
    "\n",
    "This process enables efficient document retrieval by transforming PDF content into embeddings, making it ready for similarity search and query handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=220)\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Load and Process the PDF**\n",
    "\n",
    "In this section, we load and process the PDF document to create a vector store for document retrieval.  \n",
    "\n",
    "### **Steps:**\n",
    "1. **Load and Process the PDF**:  \n",
    "   - The PDF document is processed by calling the `process_pdf` function, which loads the document, splits it into chunks, generates embeddings, and stores them in a FAISS vector store.\n",
    "\n",
    "2. **Create the Retriever**:  \n",
    "   - The vector store is converted into a retriever with the specified number of top document results (`k=5`). The retriever will fetch the most relevant document chunks based on similarity to a given query.\n",
    "\n",
    "This step prepares the system for efficient document retrieval by transforming the PDF into a searchable vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and process the PDF\n",
    "pdf_path = \"Revolutionizing YouTube Video Summaries.pdf\"\n",
    "vectorstore = process_pdf(pdf_path)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Query Refinement Agent\n",
    "\n",
    "The **Query Refinement Agent** is designed to enhance user queries by improving clarity, specificity, and context. This ensures that document retrieval is more accurate, leading to better results in the downstream retrieval and response generation processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRefinementAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            role=\"Query Refiner\",\n",
    "            backstory=\"I enhance user queries by clarifying intent and adding relevant context to improve search accuracy.\",\n",
    "            goal=\"Refine the user's query to retrieve the most relevant document excerpts.\"\n",
    "        )\n",
    "        self.llm = llm\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        query = task.description\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an AI assistant that refines user queries for document retrieval. \"\n",
    "            \"Rephrase the query to make it clearer and more specific without altering its intent.\\n\\n\"\n",
    "            f\"Original Query: {query}\\n\\n\"\n",
    "            \"Refined Query:\"\n",
    "        )\n",
    "\n",
    "        refined_query = self.llm.call([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ])\n",
    "\n",
    "        return refined_query.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2  Retrieval Agent**\n",
    "\n",
    "In this section, we define and initialize the **PDFRetrievalAgent**, which is responsible for retrieving relevant excerpts from the PDF based on a user query.\n",
    "\n",
    "### **Steps:**\n",
    "1. **Define the Retrieval Agent Class**:  \n",
    "   - The `PDFRetrievalAgent` class inherits from the `Agent` class and is configured with a specific role, backstory, and goal. The agent's purpose is to fetch the most relevant document excerpts based on a user query.\n",
    "   \n",
    "2. **Initialize the Retriever**:  \n",
    "   - The agent stores the `retriever` (created in the previous step) as a private attribute, enabling the agent to perform efficient document retrieval.\n",
    "\n",
    "3. **Execute the Retrieval Task**:  \n",
    "   - The `execute_task` method takes a user query, retrieves the top documents from the vector store, and concatenates the text of the relevant excerpts for further processing.\n",
    "\n",
    "4. **Initialize the Retrieval Agent**:  \n",
    "   - The `pdf_retrieval_agent` is initialized and ready to retrieve document excerpts based on user input.\n",
    "\n",
    "This step defines the agent responsible for the retrieval process, ensuring that relevant document excerpts are fetched based on similarity to a user’s query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Retrieval Agent\n",
    "class PDFRetrievalAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            role=\"PDF Retriever\",\n",
    "            backstory=\"I retrieve relevant information from research papers stored as PDFs.\",\n",
    "            goal=\"Fetch the most relevant document excerpts based on a user query.\"\n",
    "        )\n",
    "        self._retriever = retriever  # Store the retriever as a private attribute\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        query = task.description\n",
    "        docs = self._retriever.invoke(query)\n",
    "        retrieved_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        return retrieved_text\n",
    "\n",
    "# Initialize the Agent\n",
    "pdf_retrieval_agent = PDFRetrievalAgent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. LLM Agent**\n",
    "\n",
    "In this section, we define and initialize the **LLMAgent**, which processes the retrieved document excerpts and generates meaningful insights based on them.\n",
    "\n",
    "### **Steps:**\n",
    "1. **Define the LLM Agent Class**:  \n",
    "   - The `LLMAgent` class inherits from the `Agent` class and is configured with a specific role, backstory, and goal. The agent's purpose is to analyze and summarize the document excerpts retrieved by the **PDFRetrievalAgent** and generate a clear response.\n",
    "\n",
    "2. **Initialize the LLM Model**:  \n",
    "   - The agent is initialized with an LLM model, which will be used to process the retrieved text and generate answers.\n",
    "\n",
    "3. **Execute the Task**:  \n",
    "   - The `execute_task` method takes the retrieved text (context) and the user query (task description). It first ensures that the context is valid. If no valid context is provided, the agent returns a default response: \"No relevant information found in the document.\"\n",
    "   - The **system prompt** is constructed to instruct the LLM to answer strictly based on the provided document excerpts, avoiding any external knowledge.\n",
    "   - The agent then invokes the LLM model to generate a response based on the provided query and context.\n",
    "\n",
    "4. **Generate the Response**:  \n",
    "   - The LLM processes the system prompt and the user query, generating a response grounded only in the retrieved document content.\n",
    "\n",
    "This step defines the agent responsible for summarizing and interpreting the retrieved text, ensuring that the responses are strictly based on the document content and not external information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            role=\"LLM Processor\",\n",
    "            backstory=\"I analyze and refine retrieved excerpts to generate meaningful insights.\",\n",
    "            goal=\"Summarize and interpret retrieved text to provide a clear response.\"\n",
    "        )\n",
    "        self.llm = llm\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        if not context or not isinstance(context, str) or len(context.strip()) == 0:\n",
    "            return {\"generated_response\": \"No relevant information found.\", \"retrieved_text\": \"\"}\n",
    "        \n",
    "        system_prompt = (\n",
    "            \"You are an AI assistant that answers ONLY based on the provided document excerpts. \"\n",
    "            \"Do not use external knowledge. If the answer is not found, reply with 'Not found in the document.'\\n\\n\"\n",
    "            \"DOCUMENT EXCERPTS:\\n\" + context\n",
    "        )\n",
    "\n",
    "        generated_response = self.llm.call([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": task.description}\n",
    "        ])\n",
    "\n",
    "        #return generated_response\n",
    "        return json.dumps({  \"generated_response\": generated_response.strip(),     \"retrieved_text\": context  })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Response Validation Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseVerificationAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            role=\"Response Verifier\",\n",
    "            backstory=\"I ensure that the generated response is strictly based on retrieved document excerpts.\",\n",
    "            goal=\"Validate whether the response is properly grounded in the provided excerpts.\"\n",
    "        )\n",
    "        self.llm = llm\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        \n",
    "        refined_context=json.loads(context)\n",
    "        retrieved_text = refined_context.get(\"retrieved_text\", \"\")\n",
    "        generated_response = refined_context.get(\"generated_response\", \"\")\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an AI assistant that verifies whether a generated response is properly supported \"\n",
    "            \"by the given document excerpts. Your task is to analyze the response and check if it is \"\n",
    "            \"grounded in the provided text.\\n\\n\"\n",
    "            \"DOCUMENT EXCERPTS:\\n\"\n",
    "            f\"{retrieved_text}\\n\\n\"\n",
    "            \"GENERATED RESPONSE:\\n\"\n",
    "            f\"{generated_response}\\n\\n\"\n",
    "            \"Verification Output:\\n\"\n",
    "            \"- If the response is fully supported by the excerpts, reply with: 'Verified ✅'\\n\"\n",
    "            \"- If the response includes information not found in the excerpts, highlight unsupported parts.\"\n",
    "        )\n",
    "        print(\"system_prompt\",system_prompt)\n",
    "        print(\"generated_response\",generated_response)\n",
    "        verification_result = self.llm.call([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": generated_response}\n",
    "        ])\n",
    "\n",
    "        return verification_result.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Initialize the Agents**\n",
    "\n",
    "In this section, we initialize the **LLM model** and create the associated **LLMAgent** responsible for processing the retrieved text and generating meaningful responses.\n",
    "\n",
    "### **Steps:**\n",
    "1. **Initialize the LLM Model**:  \n",
    "   - We initialize the LLM model by specifying the model identifier (`ollama/deepseek-r1:1.5b`) and the base URL (`http://localhost:11434`) where the model is hosted. This model will be used by the **LLMAgent** to generate responses.\n",
    "\n",
    "2. **Create the LLMAgent**:  \n",
    "   - The `LLMAgent` is initialized with the LLM model, which enables it to process the retrieved document excerpts.\n",
    "\n",
    "3. **Define the LLM Task**:  \n",
    "   - A `Task` is created for the **LLMAgent** with the following attributes:  \n",
    "     - **Description**: \"Analyze and summarize the retrieved text.\"  \n",
    "     - **Expected Output**: \"A well-structured response based on the retrieved excerpts.\"  \n",
    "     - **Agent**: The agent responsible for this task is the `LLMAgent` initialized earlier.\n",
    "\n",
    "This step sets up the agent that will analyze and summarize the retrieved document content, preparing it for the next steps in the system’s execution pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Example Usage**\n",
    "\n",
    "In this section, we demonstrate how the agents work together to handle a user query. The process involves querying the **PDFRetrievalAgent** for relevant document excerpts and then passing the results to the **LLMAgent** for summarization.\n",
    "\n",
    "### **Steps:**\n",
    "1. **Define the Query**:  \n",
    "   - A user query, such as \"What are the prerequisites to run the project?\", is defined to be processed by the system.\n",
    "\n",
    "2. **Create the Retrieval Task**:  \n",
    "   - A task is created for the **PDFRetrievalAgent** with the query as its description. The expected output is \"Relevant excerpts from the PDF.\"\n",
    "\n",
    "3. **Define the Crew**:  \n",
    "   - A **Crew** is initialized to manage the execution of the agents and tasks.  \n",
    "     - **Agents**: Both the **PDFRetrievalAgent** and the **LLMAgent** are included in the crew.  \n",
    "     - **Tasks**: The task for **PDFRetrievalAgent** and the summarization task for **LLMAgent** are included.  \n",
    "     - **Context Flow**: Defines the flow of context between tasks. Here, the results of the **retrieval_task** will be passed to the **llm_task** for further processing.\n",
    "\n",
    "4. **Execute the Crew**:  \n",
    "   - The `crew.kickoff()` function initiates the tasks, and the agents execute their respective roles sequentially.  \n",
    "   - The result of the execution is printed, showing the raw output.\n",
    "\n",
    "This part demonstrates how the agents and tasks are orchestrated to handle the user query, retrieve relevant information, and generate a meaningful response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_prompt You are an AI assistant that verifies whether a generated response is properly supported by the given document excerpts. Your task is to analyze the response and check if it is grounded in the provided text.\n",
      "\n",
      "DOCUMENT EXCERPTS:\n",
      "Image created using napkin.ai Introduction How many times have you sat through a really long YouTube video just to find the answer to one specific question? Or watched an entire video for just a couple of key words from the author? Whether you’re a student trying to extract valuable knowledge from a lecture, a content creator analyzing trends, or a professional seeking quick answers, the process can be tedious and time-consuming. Enter the enhanced Retrieval-Augmented Generation (RAG) system, powered by\n",
      "video transcript using the YoutubeLoader.from_youtube_url method. It then processes the transcript by splitting it into smaller, manageable chunks using the RecursiveCharacterTextSplitter, which divides the text into segments of 512 characters. The resulting chunks are embedded using the HuggingFaceEmbeddings model (specifically, 'all-MiniLM-L6-v2'), which transforms the text into numerical representations suitable for machine learning tasks. These embeddings are then stored in a FAISS vector database,\n",
      "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'     embeddings = HuggingFaceEmbeddings(model_name=embedding_model)      db = FAISS.from_documents(split_docs, embeddings)     return db process_youtube_url(youtube_url) function takes a YouTube URL as input and extracts the video transcript using the YoutubeLoader.from_youtube_url method. It then processes the transcript by splitting it into smaller, manageable chunks using the RecursiveCharacterTextSplitter, which divides the text into\n",
      "from a lecture, a content creator analyzing trends, or a professional seeking quick answers, the process can be tedious and time-consuming. Enter the enhanced Retrieval-Augmented Generation (RAG) system, powered by cutting-edge AI tools like LangChain, Llama 3.2, and Gradio. This system transforms how you interact with YouTube videos by offering two key functionalities: summarization and interactive Q&A. Not only can it distill lengthy videos into concise summaries, but it also allows you to ask specific\n",
      "loader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=False)     documents = loader.load()      splitter = RecursiveCharacterTextSplitter(         chunk_size=512,         chunk_overlap=256,         separators=[\"\\n\\n\", \"\\n\", \" \"]     )      split_docs = splitter.split_documents(documents)      embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'     embeddings = HuggingFaceEmbeddings(model_name=embedding_model)      db = FAISS.from_documents(split_docs, embeddings)     return db\n",
      "\n",
      "GENERATED RESPONSE:\n",
      "<think>\n",
      "Okay, I need to analyze and summarize the provided document excerpt. Let me read through it carefully.\n",
      "\n",
      "The user provided an introduction that talks about how people spend a lot of time reviewing YouTube videos for specific answers or trends. It mentions different roles like students extracting knowledge, content creators analyzing trends, and professionals seeking quick answers. The document then introduces the enhanced Retrieval-Augmented Generation (RAG) system using the YoutubeLoader.from_youtube_url method to extract transcripts.\n",
      "\n",
      "The RAG system splits the transcript into manageable chunks using RecursiveCharacterTextSplitter, which divides text into 512-character segments. It then transforms these texts into embeddings using HuggingFaceEmbeddings model and stores them in a FAISS vector database for efficient machine learning tasks.\n",
      "\n",
      "Next, it explains that the system offers two functionalities: summarization by distilling lengthy videos into concise summaries, and interactive Q&A allowing users to ask specific questions with embedded embeddings. It also mentions collaboration tools like LangChain, Llama 3.2, and Gradio, which together enhance the RAG process.\n",
      "\n",
      "The document ends with a summary highlighting that RAG can save time on processing YouTube content by providing structured summaries and enabling quick interactions with video content.\n",
      "</think>\n",
      "\n",
      "The provided document introduces an enhanced Retrieval-Augmented Generation (RAG) system using the YoutubeLoader method to extract video transcripts. The system splits these texts into manageable chunks of 512 characters, converting them into embeddings for efficient machine learning tasks. It also offers features like summarization and interactive Q&A, powered by cutting-edge AI tools such as LangChain, Llama 3.2, and Gradio. This RAG system efficiently processes YouTube content, saving time through structured summaries and quick interactions.\n",
      "\n",
      "**Summary:**\n",
      "The document presents an RAG system that leverages YouTube transcripts from the YoutubeLoader method. It splits texts into 512-character chunks for embeddings, enabling efficient machine learning tasks. The system includes features like summarization and interactive Q&A, powered by LangChain, Llama 3.2, and Gradio. This enhanced process saves time by providing structured summaries and quick video interactions.\n",
      "\n",
      "The final answer is: \n",
      "\n",
      "**Summary:** The document introduces an RAG system that processes YouTube transcripts into manageable chunks using RecursiveCharacterTextSplitter for embeddings. It offers summarization and interactive Q&A, powered by AI tools like LangChain, Llama 3.2, and Gradio, enhancing efficiency with structured summaries and quick interactions.\n",
      "\n",
      "Verification Output:\n",
      "- If the response is fully supported by the excerpts, reply with: 'Verified ✅'\n",
      "- If the response includes information not found in the excerpts, highlight unsupported parts.\n",
      "generated_response <think>\n",
      "Okay, I need to analyze and summarize the provided document excerpt. Let me read through it carefully.\n",
      "\n",
      "The user provided an introduction that talks about how people spend a lot of time reviewing YouTube videos for specific answers or trends. It mentions different roles like students extracting knowledge, content creators analyzing trends, and professionals seeking quick answers. The document then introduces the enhanced Retrieval-Augmented Generation (RAG) system using the YoutubeLoader.from_youtube_url method to extract transcripts.\n",
      "\n",
      "The RAG system splits the transcript into manageable chunks using RecursiveCharacterTextSplitter, which divides text into 512-character segments. It then transforms these texts into embeddings using HuggingFaceEmbeddings model and stores them in a FAISS vector database for efficient machine learning tasks.\n",
      "\n",
      "Next, it explains that the system offers two functionalities: summarization by distilling lengthy videos into concise summaries, and interactive Q&A allowing users to ask specific questions with embedded embeddings. It also mentions collaboration tools like LangChain, Llama 3.2, and Gradio, which together enhance the RAG process.\n",
      "\n",
      "The document ends with a summary highlighting that RAG can save time on processing YouTube content by providing structured summaries and enabling quick interactions with video content.\n",
      "</think>\n",
      "\n",
      "The provided document introduces an enhanced Retrieval-Augmented Generation (RAG) system using the YoutubeLoader method to extract video transcripts. The system splits these texts into manageable chunks of 512 characters, converting them into embeddings for efficient machine learning tasks. It also offers features like summarization and interactive Q&A, powered by cutting-edge AI tools such as LangChain, Llama 3.2, and Gradio. This RAG system efficiently processes YouTube content, saving time through structured summaries and quick interactions.\n",
      "\n",
      "**Summary:**\n",
      "The document presents an RAG system that leverages YouTube transcripts from the YoutubeLoader method. It splits texts into 512-character chunks for embeddings, enabling efficient machine learning tasks. The system includes features like summarization and interactive Q&A, powered by LangChain, Llama 3.2, and Gradio. This enhanced process saves time by providing structured summaries and quick video interactions.\n",
      "\n",
      "The final answer is: \n",
      "\n",
      "**Summary:** The document introduces an RAG system that processes YouTube transcripts into manageable chunks using RecursiveCharacterTextSplitter for embeddings. It offers summarization and interactive Q&A, powered by AI tools like LangChain, Llama 3.2, and Gradio, enhancing efficiency with structured summaries and quick interactions.\n",
      "<think>\n",
      "The user has provided two excerpts from a document and an AI response that needs verification. The AI response is considered fully supported by the excerpts, so no corrections or further analysis are needed.\n",
      "</think>\n",
      "\n",
      "**Response:**\n",
      "\n",
      "Yes, the AI response is fully supported by the excerpts provided.\n",
      "\n",
      "**Summary:** \n",
      "\n",
      "The AI response provides a clear and concise summary of the content, highlighting key points such as how YouTube transcripts are processed using the YoutubeLoader method, the use of RecursiveCharacterTextSplitter for chunking, HuggingFaceEmbeddings for embeddings, and FAISS vector databases. It also mentions the features of summarization and interactive Q&A, along with collaboration tools like LangChain, Llama 3.2, and Gradio.\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "Yes, the AI response is fully supported by the provided excerpts.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"ollama/deepseek-r1:1.5b\", base_url=\"http://localhost:11434\")\n",
    "# Example usage\n",
    "query = \"what are the Prerequisites to Run the Project\"\n",
    "query_refinement_agent = QueryRefinementAgent(llm)\n",
    "query_refinement_task = Task(\n",
    "    description=query,\n",
    "    expected_output=\"A well-structured and precise query for document retrieval.\",\n",
    "    agent=query_refinement_agent\n",
    ")\n",
    "\n",
    "retrieval_task = Task(\n",
    "    description=\"Retrieve the most relevant excerpts from the uploaded PDF based on the refined query.\",\n",
    "    expected_output=\"A list of the most relevant document excerpts.\",\n",
    "    agent=pdf_retrieval_agent\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "llm_agent = LLMAgent(llm)\n",
    "llm_task = Task(\n",
    "    description=\"Analyze and summarize the retrieved text.\",\n",
    "    expected_output=\"A well-structured response based on the retrieved excerpts.\",\n",
    "    agent=llm_agent\n",
    ")\n",
    "\n",
    "response_verification_agent=ResponseVerificationAgent(llm)\n",
    "response_verification_task = Task(\n",
    "    description=\"Verify if the generated response is based solely on the retrieved document excerpts.\",\n",
    "    expected_output=\"Verification result indicating whether the response is supported by the excerpts.\",\n",
    "    agent=response_verification_agent, context=[llm_task]\n",
    ")\n",
    "\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[query_refinement_agent, pdf_retrieval_agent, llm_agent,response_verification_agent],\n",
    "    tasks=[query_refinement_task, retrieval_task,llm_task,response_verification_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential,\n",
    "    context_flow={\n",
    "        retrieval_task: query_refinement_task,\n",
    "        llm_task: retrieval_task,\n",
    "        response_verification_task: llm_task  # ✅ Now contains both retrieved_text and generated_response\n",
    "    }\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "print(result.raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_output = llm_task.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Description: Analyze and summarize the retrieved text.\n",
      "Task Summary: Analyze and summarize the retrieved text....\n",
      "Raw Output: {\"generated_response\": \"<think>\\nOkay, I need to analyze and summarize the provided document excerpt. Let me read through it carefully.\\n\\nThe user provided an introduction that talks about how people spend a lot of time reviewing YouTube videos for specific answers or trends. It mentions different roles like students extracting knowledge, content creators analyzing trends, and professionals seeking quick answers. The document then introduces the enhanced Retrieval-Augmented Generation (RAG) system using the YoutubeLoader.from_youtube_url method to extract transcripts.\\n\\nThe RAG system splits the transcript into manageable chunks using RecursiveCharacterTextSplitter, which divides text into 512-character segments. It then transforms these texts into embeddings using HuggingFaceEmbeddings model and stores them in a FAISS vector database for efficient machine learning tasks.\\n\\nNext, it explains that the system offers two functionalities: summarization by distilling lengthy videos into concise summaries, and interactive Q&A allowing users to ask specific questions with embedded embeddings. It also mentions collaboration tools like LangChain, Llama 3.2, and Gradio, which together enhance the RAG process.\\n\\nThe document ends with a summary highlighting that RAG can save time on processing YouTube content by providing structured summaries and enabling quick interactions with video content.\\n</think>\\n\\nThe provided document introduces an enhanced Retrieval-Augmented Generation (RAG) system using the YoutubeLoader method to extract video transcripts. The system splits these texts into manageable chunks of 512 characters, converting them into embeddings for efficient machine learning tasks. It also offers features like summarization and interactive Q&A, powered by cutting-edge AI tools such as LangChain, Llama 3.2, and Gradio. This RAG system efficiently processes YouTube content, saving time through structured summaries and quick interactions.\\n\\n**Summary:**\\nThe document presents an RAG system that leverages YouTube transcripts from the YoutubeLoader method. It splits texts into 512-character chunks for embeddings, enabling efficient machine learning tasks. The system includes features like summarization and interactive Q&A, powered by LangChain, Llama 3.2, and Gradio. This enhanced process saves time by providing structured summaries and quick video interactions.\\n\\nThe final answer is: \\n\\n**Summary:** The document introduces an RAG system that processes YouTube transcripts into manageable chunks using RecursiveCharacterTextSplitter for embeddings. It offers summarization and interactive Q&A, powered by AI tools like LangChain, Llama 3.2, and Gradio, enhancing efficiency with structured summaries and quick interactions.\", \"retrieved_text\": \"Image created using napkin.ai Introduction How many times have you sat through a really long YouTube video just to find the answer to one specific question? Or watched an entire video for just a couple of key words from the author? Whether you\\u2019re a student trying to extract valuable knowledge from a lecture, a content creator analyzing trends, or a professional seeking quick answers, the process can be tedious and time-consuming. Enter the enhanced Retrieval-Augmented Generation (RAG) system, powered by\\nvideo transcript using the YoutubeLoader.from_youtube_url method. It then processes the transcript by splitting it into smaller, manageable chunks using the RecursiveCharacterTextSplitter, which divides the text into segments of 512 characters. The resulting chunks are embedded using the HuggingFaceEmbeddings model (specifically, 'all-MiniLM-L6-v2'), which transforms the text into numerical representations suitable for machine learning tasks. These embeddings are then stored in a FAISS vector database,\\nembedding_model = 'sentence-transformers/all-MiniLM-L6-v2'     embeddings = HuggingFaceEmbeddings(model_name=embedding_model)      db = FAISS.from_documents(split_docs, embeddings)     return db process_youtube_url(youtube_url) function takes a YouTube URL as input and extracts the video transcript using the YoutubeLoader.from_youtube_url method. It then processes the transcript by splitting it into smaller, manageable chunks using the RecursiveCharacterTextSplitter, which divides the text into\\nfrom a lecture, a content creator analyzing trends, or a professional seeking quick answers, the process can be tedious and time-consuming. Enter the enhanced Retrieval-Augmented Generation (RAG) system, powered by cutting-edge AI tools like LangChain, Llama 3.2, and Gradio. This system transforms how you interact with YouTube videos by offering two key functionalities: summarization and interactive Q&A. Not only can it distill lengthy videos into concise summaries, but it also allows you to ask specific\\nloader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=False)     documents = loader.load()      splitter = RecursiveCharacterTextSplitter(         chunk_size=512,         chunk_overlap=256,         separators=[\\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\"]     )      split_docs = splitter.split_documents(documents)      embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'     embeddings = HuggingFaceEmbeddings(model_name=embedding_model)      db = FAISS.from_documents(split_docs, embeddings)     return db\"}\n",
      "JSON Output: {'generated_response': '<think>\\nOkay, I need to analyze and summarize the provided document excerpt. Let me read through it carefully.\\n\\nThe user provided an introduction that talks about how people spend a lot of time reviewing YouTube videos for specific answers or trends. It mentions different roles like students extracting knowledge, content creators analyzing trends, and professionals seeking quick answers. The document then introduces the enhanced Retrieval-Augmented Generation (RAG) system using the YoutubeLoader.from_youtube_url method to extract transcripts.\\n\\nThe RAG system splits the transcript into manageable chunks using RecursiveCharacterTextSplitter, which divides text into 512-character segments. It then transforms these texts into embeddings using HuggingFaceEmbeddings model and stores them in a FAISS vector database for efficient machine learning tasks.\\n\\nNext, it explains that the system offers two functionalities: summarization by distilling lengthy videos into concise summaries, and interactive Q&A allowing users to ask specific questions with embedded embeddings. It also mentions collaboration tools like LangChain, Llama 3.2, and Gradio, which together enhance the RAG process.\\n\\nThe document ends with a summary highlighting that RAG can save time on processing YouTube content by providing structured summaries and enabling quick interactions with video content.\\n</think>\\n\\nThe provided document introduces an enhanced Retrieval-Augmented Generation (RAG) system using the YoutubeLoader method to extract video transcripts. The system splits these texts into manageable chunks of 512 characters, converting them into embeddings for efficient machine learning tasks. It also offers features like summarization and interactive Q&A, powered by cutting-edge AI tools such as LangChain, Llama 3.2, and Gradio. This RAG system efficiently processes YouTube content, saving time through structured summaries and quick interactions.\\n\\n**Summary:**\\nThe document presents an RAG system that leverages YouTube transcripts from the YoutubeLoader method. It splits texts into 512-character chunks for embeddings, enabling efficient machine learning tasks. The system includes features like summarization and interactive Q&A, powered by LangChain, Llama 3.2, and Gradio. This enhanced process saves time by providing structured summaries and quick video interactions.\\n\\nThe final answer is: \\n\\n**Summary:** The document introduces an RAG system that processes YouTube transcripts into manageable chunks using RecursiveCharacterTextSplitter for embeddings. It offers summarization and interactive Q&A, powered by AI tools like LangChain, Llama 3.2, and Gradio, enhancing efficiency with structured summaries and quick interactions.', 'retrieved_text': 'Image created using napkin.ai Introduction How many times have you sat through a really long YouTube video just to find the answer to one specific question? Or watched an entire video for just a couple of key words from the author? Whether you’re a student trying to extract valuable knowledge from a lecture, a content creator analyzing trends, or a professional seeking quick answers, the process can be tedious and time-consuming. Enter the enhanced Retrieval-Augmented Generation (RAG) system, powered by\\nvideo transcript using the YoutubeLoader.from_youtube_url method. It then processes the transcript by splitting it into smaller, manageable chunks using the RecursiveCharacterTextSplitter, which divides the text into segments of 512 characters. The resulting chunks are embedded using the HuggingFaceEmbeddings model (specifically, \\'all-MiniLM-L6-v2\\'), which transforms the text into numerical representations suitable for machine learning tasks. These embeddings are then stored in a FAISS vector database,\\nembedding_model = \\'sentence-transformers/all-MiniLM-L6-v2\\'     embeddings = HuggingFaceEmbeddings(model_name=embedding_model)      db = FAISS.from_documents(split_docs, embeddings)     return db process_youtube_url(youtube_url) function takes a YouTube URL as input and extracts the video transcript using the YoutubeLoader.from_youtube_url method. It then processes the transcript by splitting it into smaller, manageable chunks using the RecursiveCharacterTextSplitter, which divides the text into\\nfrom a lecture, a content creator analyzing trends, or a professional seeking quick answers, the process can be tedious and time-consuming. Enter the enhanced Retrieval-Augmented Generation (RAG) system, powered by cutting-edge AI tools like LangChain, Llama 3.2, and Gradio. This system transforms how you interact with YouTube videos by offering two key functionalities: summarization and interactive Q&A. Not only can it distill lengthy videos into concise summaries, but it also allows you to ask specific\\nloader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=False)     documents = loader.load()      splitter = RecursiveCharacterTextSplitter(         chunk_size=512,         chunk_overlap=256,         separators=[\"\\\\n\\\\n\", \"\\\\n\", \" \"]     )      split_docs = splitter.split_documents(documents)      embedding_model = \\'sentence-transformers/all-MiniLM-L6-v2\\'     embeddings = HuggingFaceEmbeddings(model_name=embedding_model)      db = FAISS.from_documents(split_docs, embeddings)     return db'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(f\"Task Description: {task_output.description}\")\n",
    "print(f\"Task Summary: {task_output.summary}\")\n",
    "print(f\"Raw Output: {task_output.raw}\")\n",
    "print(f\"JSON Output: {json.loads(task_output.raw)}\")\n",
    "if task_output.json_dict:\n",
    "    print(f\"JSON Output: {json.dumps(task_output.json_dict, indent=2)}\")\n",
    "if task_output.pydantic:\n",
    "    print(f\"Pydantic Output: {task_output.pydantic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Extract and Remove `<think>` Content**\n",
    "\n",
    "In this section, we process the raw result output by extracting and removing specific content enclosed within `<think>` tags. This ensures that only the relevant parts of the response are shown to the user.\n",
    "\n",
    "### **Steps:**\n",
    "1. **Extract Content Inside `<think>` Tags**:  \n",
    "   - The `extract_think_content` function uses regular expressions to find and extract content within `<think>` and `</think>` tags. If content is found, it is returned; otherwise, an empty string is returned.\n",
    "\n",
    "2. **Remove Content Inside `<think>` Tags**:  \n",
    "   - The `remove_think_content` function uses regular expressions to remove everything between `<think>` and `</think>` tags. This is useful to clean up the result and ensure only the relevant content remains.\n",
    "\n",
    "3. **Process the Raw Result**:  \n",
    "   - The `think_content` variable stores any extracted content from the `<think>` tags, while the `response` variable stores the cleaned response with the `<think>` content removed.\n",
    "\n",
    "4. **Print the Final Response**:  \n",
    "   - The cleaned response (without `<think>` content) is printed, ensuring the final answer is concise and relevant.\n",
    "\n",
    "This step processes the raw response to clean and refine the output, making it ready for presentation to the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_think_content(text):\n",
    "    match = re.search(r'<think>(.*?)</think>', text, flags=re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "# Function to remove everything between <think> and </think> tags\n",
    "def remove_think_content(text):\n",
    "    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "think_content=extract_think_content(result.raw)\n",
    "response=remove_think_content(result.raw)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "\n",
    "This notebook demonstrates the creation and orchestration of agents to process a PDF document and generate meaningful insights from user queries. The system performs the following key steps:\n",
    "\n",
    "1. **PDF Processing**: The PDF document is loaded, split into manageable chunks, and stored in a FAISS vector store for efficient retrieval using embeddings.\n",
    "2. **Retrieval Agent**: A **PDFRetrievalAgent** retrieves the most relevant document excerpts based on the user’s query.\n",
    "3. **LLM Processing**: The **LLMAgent** processes the retrieved text using a pre-trained language model, generating a well-structured response.\n",
    "4. **Task Execution**: The **Crew** orchestrates the execution of these agents, ensuring sequential processing of the tasks (retrieving and summarizing the document content).\n",
    "5. **Result Refining**: The raw response is refined by extracting or removing content enclosed in `<think>` tags, providing a clean and relevant response to the user.\n",
    "\n",
    "This system efficiently combines retrieval-based and generation-based techniques to answer user queries, making it a robust tool for document analysis and summarization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
