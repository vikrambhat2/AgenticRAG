{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Agent RAG for Document-Based Question Answering**  \n",
    "\n",
    "## **Introduction**  \n",
    "This notebook demonstrates a **Multi-Agent Retrieval-Augmented Generation (RAG) system** using **CrewAI** to efficiently query and analyze documents. It employs a structured workflow where multiple agents collaborate to refine queries, retrieve relevant information, generate responses, and verify their accuracy.  \n",
    "\n",
    "### **Key Features:**  \n",
    "1. **Multi-format Document Processing** – Supports PDFs, TXT, DOCX, and HTML, extracting and chunking content for efficient retrieval.  \n",
    "2. **Vector Search with FAISS** – Uses `HuggingFaceEmbeddings` to embed document chunks and retrieve relevant excerpts.  \n",
    "3. **Agent-Based Workflow** – Dedicated agents for query refinement, document retrieval, response generation, and verification ensure structured processing.  \n",
    "4. **CrewAI Orchestration** – Implements a sequential process to enhance retrieval accuracy and response reliability.  \n",
    "5. **Local LLM Execution with Ollama** – Utilizes `ollama/deepseek-r1:1.5b` for all LLM-driven tasks, enabling offline execution without cloud dependencies.  \n",
    "6. **Customizable and Extensible** – Easily adaptable to different LLMs, retrieval techniques, and document types.  \n",
    "\n",
    "### **Workflow Overview:**  \n",
    "- Processes and embeds documents using FAISS for efficient search.  \n",
    "- Retrieves the top 5 most relevant excerpts based on a refined user query.  \n",
    "- Enhances query clarity for improved document retrieval.  \n",
    "- Generates structured responses strictly based on retrieved content.  \n",
    "- Verifies responses to ensure they are fully grounded in the provided excerpts.  \n",
    "- Uses CrewAI to seamlessly manage agent interactions and context flow.  \n",
    "- Outputs structured results while cleaning agents responses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import Required Libraries**  \n",
    "\n",
    "This section imports the essential libraries for building an **Agentic RAG system** to process PDFs and answer user queries.  \n",
    "\n",
    "- **CrewAI** (`Agent, Task, Crew, Process, LLM`) – Manages multi-agent workflows, orchestrating query refinement, retrieval, and response validation.  \n",
    "- **LangChain** (`RecursiveCharacterTextSplitter, FAISS, HuggingFaceEmbeddings`) – Splits documents into chunks, embeds them using HuggingFace models, and enables vector-based retrieval with FAISS.  \n",
    "- **LangChain Document Loaders** (`DirectoryLoader, TextLoader, PyPDFLoader`) – Loads documents from various formats (PDF, TXT, DOCX, HTML) for processing.  \n",
    "- **JSON** – Handles structured data output, ensuring easy parsing and analysis of results.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from crewai import LLM\n",
    "import json\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Documents Processing and Vector Store Creation**  \n",
    "\n",
    "This function processes all types of documents, splits it into smaller chunks, and creates a vector store for efficient similarity-based search.  \n",
    "The `process_all_files(directory)` function:  \n",
    "1. Loads **PDF, TXT, DOCX, and HTML** files from a directory.  \n",
    "2. Splits text into chunks using `RecursiveCharacterTextSplitter`.  \n",
    "3. Embeds chunks with `HuggingFaceEmbeddings`.  \n",
    "4. Stores embeddings in a **FAISS vector database**.  \n",
    "5. Returns a retriever for efficient document search.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_all_files(directory):\n",
    "    loaders = [\n",
    "        DirectoryLoader(directory, glob=\"**/*.html\",show_progress=True),\n",
    "        DirectoryLoader(directory, glob=\"**/*.pdf\",show_progress=True, loader_cls=PyPDFLoader),\n",
    "        DirectoryLoader(directory, glob=\"**/*.txt\",show_progress=True, loader_cls=TextLoader),\n",
    "        DirectoryLoader(directory, glob=\"**/*.docx\",show_progress=True)\n",
    "    ]\n",
    "    documents=[]\n",
    "    for loader in loaders:\n",
    "        data =loader.load()\n",
    "        documents.extend(data)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=150)\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 **Loading and Processing Documents**\n",
    "\n",
    "- **Directory Setup**: The directory containing documents is specified as `\"data/\"`.  \n",
    "- **Document Processing**: The `process_all_files(directory)` method is called to load, split, embed, and store the document chunks in a **FAISS vector store**.  \n",
    "- **Retriever Initialization**: The vector store is then converted into a retriever with the top 5 relevant document excerpts being retrieved using `search_kwargs={\"k\": 5}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and process the PDF\n",
    "directory=\"data/\"\n",
    "vectorstore=process_all_files(directory)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.**Creating Agents for the Workflow**\n",
    "\n",
    "In this section, we will define the agents that will handle different tasks within the retrieval-augmented generation (RAG) workflow. Each agent will be responsible for a specific step, such as refining user queries, retrieving relevant document excerpts, generating responses, and verifying the accuracy of those responses. These agents work together in a sequential flow to ensure that the entire process from query refinement to response verification is seamless and efficient. Let's start by creating the **QueryRefinementAgent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 [QueryRefinementAgent](#)**\n",
    "\n",
    "The **QueryRefinementAgent** is responsible for improving the clarity and specificity of user queries to enhance document retrieval accuracy. By refining the user’s original query, this agent ensures that the most relevant document excerpts are retrieved.\n",
    "\n",
    "#### **Key Features**:\n",
    "- **Role**: Refines user queries to ensure clarity without changing their original intent.\n",
    "- **Backstory**: Aids in improving search accuracy by clarifying the user's query.\n",
    "- **Goal**: To refine queries for more relevant document retrieval.\n",
    "\n",
    "#### **Workflow**:\n",
    "1. The agent receives the user’s query.\n",
    "2. It uses an LLM to rephrase the query and make it more specific.\n",
    "3. The refined query is then passed on for document retrieval.\n",
    "\n",
    "This agent is the first step in the sequential workflow, setting the stage for improved document retrieval in later stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRefinementAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            role=\"Query Refiner\",\n",
    "            backstory=\"I enhance user queries by clarifying intent and adding relevant context to improve search accuracy.\",\n",
    "            goal=\"Refine the user's query to retrieve the most relevant document excerpts.\"\n",
    "        )\n",
    "        self.llm = llm\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        query = task.description\n",
    "\n",
    "        system_prompt =  (\n",
    "        \"You are an AI assistant that refines user queries for document retrieval. \"\n",
    "        \"Rephrase the query to be clearer and more specific while keeping it concise. \"\n",
    "        \"Return only the refined query without any explanations or additional text.\\n\\n\"\n",
    "        f\"Original Query: {query}\\n\\n\"\n",
    "        \"Refined Query:\"\n",
    "    )\n",
    "\n",
    "        refined_query = self.llm.call([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ])\n",
    "\n",
    "        return refined_query.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2  [RetrievalAgent](#)**\n",
    "\n",
    "The **RetrievalAgent** is responsible for retrieving relevant document excerpts from the stored vector database based on a user’s query. It utilizes a retriever to search the indexed documents and fetch the most pertinent content for further processing.\n",
    "\n",
    "#### **Key Features**:\n",
    "- **Role**: Retrieves relevant document excerpts using a pre-defined retriever.\n",
    "- **Backstory**: Specializes in fetching the most relevant information from research papers or documents, particularly stored in PDF format.\n",
    "- **Goal**: To fetch the top document excerpts based on the user query for accurate response generation.\n",
    "\n",
    "#### **Workflow**:\n",
    "1. The agent receives the user query from the **QueryRefinementAgent**.\n",
    "2. It queries the vectorstore (FAISS) using the retriever to fetch relevant document excerpts.\n",
    "3. The relevant document content is returned for analysis and response generation.\n",
    "\n",
    "This agent ensures that the generated responses are directly grounded in the most relevant content, making it a key part of the retrieval process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Retrieval Agent\n",
    "class RetrievalAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            role=\"PDF Retriever\",\n",
    "            backstory=\"I retrieve relevant information from research papers stored as PDFs.\",\n",
    "            goal=\"Fetch the most relevant document excerpts based on a user query.\"\n",
    "        )\n",
    "        self._retriever = retriever  # Store the retriever as a private attribute\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        query = task.description\n",
    "        docs = self._retriever.invoke(query)\n",
    "        retrieved_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        return retrieved_text\n",
    "\n",
    "# Initialize the Agent\n",
    "retrieval_agent = RetrievalAgent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 [LLMAgent](#)**\n",
    "\n",
    "The **LLMAgent** is responsible for analyzing the retrieved document excerpts and generating meaningful insights or responses based on them. Using an LLM (Language Model), this agent interprets the retrieved content and provides a clear, structured answer to the user's query.\n",
    "\n",
    "#### **Key Features**:\n",
    "- **Role**: Processes retrieved document excerpts to generate relevant responses.\n",
    "- **Backstory**: Refines and interprets the retrieved content to summarize and generate meaningful insights.\n",
    "- **Goal**: To produce clear, accurate responses by analyzing the provided document excerpts.\n",
    "\n",
    "#### **Workflow**:\n",
    "1. The agent receives the retrieved document excerpts from the **RetrievalAgent**.\n",
    "2. It uses the LLM to generate a response based on the provided context, ensuring the response is strictly based on the content in the document excerpts.\n",
    "3. The generated response is returned along with the retrieved content for verification.\n",
    "\n",
    "This agent ensures that the response generated is relevant to the user’s query and grounded in the provided document content, making it an essential part of the response generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            role=\"LLM Processor\",\n",
    "            backstory=\"I analyze and refine retrieved excerpts to generate meaningful insights.\",\n",
    "            goal=\"Summarize and interpret retrieved text to provide a clear response.\"\n",
    "        )\n",
    "        self.llm = llm\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        if not context or not isinstance(context, str) or len(context.strip()) == 0:\n",
    "            return {\"generated_response\": \"No relevant information found.\", \"retrieved_text\": \"\"}\n",
    "        \n",
    "        system_prompt = (\n",
    "            \"You are an AI assistant that answers ONLY based on the provided document excerpts. \"\n",
    "            \"Do not use external knowledge. If the answer is not found, reply with 'Not found in the document.'\\n\\n\"\n",
    "            \"DOCUMENT EXCERPTS:\\n\" + context\n",
    "        )\n",
    "\n",
    "        generated_response = self.llm.call([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": task.description}\n",
    "        ])\n",
    "\n",
    "        #return generated_response\n",
    "        return json.dumps({  \"generated_response\": generated_response.strip(),     \"retrieved_text\": context  })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 **[ResponseVerificationAgent](#)**\n",
    "\n",
    "The **ResponseVerificationAgent** ensures that the generated response is fully supported by the retrieved document excerpts. This agent checks the alignment between the response and the content it was generated from, verifying that no external or unsupported information is included.\n",
    "\n",
    "#### **Key Features**:\n",
    "- **Role**: Verifies that the generated response is grounded in the retrieved document excerpts.\n",
    "- **Backstory**: Ensures that the final response aligns with the provided text, highlighting any unsupported claims.\n",
    "- **Goal**: To validate whether the response is properly supported by the document excerpts or flag any unsupported information.\n",
    "\n",
    "#### **Workflow**:\n",
    "1. The agent receives the generated response and the retrieved document excerpts from the **LLMAgent**.\n",
    "2. It analyzes the response to check if all information is supported by the document excerpts.\n",
    "3. The agent returns a verification result:\n",
    "   - If the response is fully supported, it returns \"Verified ✅\".\n",
    "   - If the response contains unsupported information, it highlights the unsupported parts.\n",
    "\n",
    "This agent plays a crucial role in ensuring the integrity and accuracy of the response, confirming that it is based solely on the retrieved content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseVerificationAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            role=\"Response Verifier\",\n",
    "            backstory=\"I ensure that the generated response is strictly based on retrieved document excerpts.\",\n",
    "            goal=\"Validate whether the response is properly grounded in the provided excerpts.\"\n",
    "        )\n",
    "        self.llm = llm\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        \n",
    "        refined_context=json.loads(context)\n",
    "        retrieved_text = refined_context.get(\"retrieved_text\", \"\")\n",
    "        generated_response = refined_context.get(\"generated_response\", \"\")\n",
    "        print(retrieved_text)\n",
    "        print(generated_response)\n",
    "        system_prompt = (\n",
    "            \"You are an AI assistant that verifies whether a generated response is properly supported \"\n",
    "            \"by the given document excerpts. Your task is to analyze the response and check if it is \"\n",
    "            \"grounded in the provided text.\\n\\n\"\n",
    "            \"DOCUMENT EXCERPTS:\\n\"\n",
    "            f\"{retrieved_text}\\n\\n\"\n",
    "            \"GENERATED RESPONSE:\\n\"\n",
    "            f\"{generated_response}\\n\\n\"\n",
    "            \"Verification Output:\\n\"\n",
    "            \"- If the response is fully supported by the DOCUMENT EXCERPTS, reply with: 'Verified ✅'\\n\"\n",
    "            \"- If the response includes information not found in the DOCUMENT EXCERPTS, reply with: 'Unable to Verify'.\"\n",
    "        )\n",
    "        print(\"system_prompt\",system_prompt)\n",
    "        print(\"generated_response\",generated_response)\n",
    "        verification_result = self.llm.call([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": generated_response}\n",
    "        ])\n",
    "\n",
    "        return verification_result.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.**Sequential Agent Workflow for Query Refinement, Retrieval, Response Generation, and Verification**\n",
    "This section outlines the process of initializing and executing a multi-agent workflow for querying documents, refining user input, retrieving relevant content, generating responses, and verifying the accuracy of the generated answers.\n",
    "\n",
    "\n",
    "\n",
    "#### **4.1. Initialize Local LLM (Ollama)**\n",
    "\n",
    "The **LLM** is initialized using a locally hosted model `ollama/deepseek-r1:1.5b`, which will be used by the various agents for query refinement, response generation, and verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"ollama/deepseek-r1:1.5b\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Query Refinement Agent Initialization\n",
    "The `QueryRefinementAgent` is initialized with the local LLM model. The agent's purpose is to refine user queries to enhance the accuracy of document retrieval. A task is created to process a sample query related to project prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the Prerequisites to Run the Project\"\n",
    "query_refinement_agent = QueryRefinementAgent(llm)\n",
    "query_refinement_task = Task(\n",
    "    description=query,\n",
    "    expected_output=\"A well-structured and precise query for document retrieval.\",\n",
    "    agent=query_refinement_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Retrieval Agent Task\n",
    "A retrieval task is defined where the agent fetches the most relevant excerpts from the uploaded PDFs based on the refined query. This task ensures that only the most pertinent information is retrieved from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_task = Task(\n",
    "    description=\"Retrieve the most relevant excerpts from the uploaded PDF based on the refined query.\",\n",
    "    expected_output=\"A list of the most relevant document excerpts.\",\n",
    "    agent=retrieval_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 LLM Agent Task for Response Generation\n",
    "The `LLMAgent` takes the retrieved excerpts and generates a well-structured response based solely on the document content. This agent ensures that the response is meaningful and interprets the retrieved information effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_agent = LLMAgent(llm)\n",
    "llm_task = Task(\n",
    "    description=\"Analyze and summarize the retrieved text.\",\n",
    "    expected_output=\"A well-structured response based on the retrieved excerpts.\",\n",
    "    agent=llm_agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5. Response Verification Agent Task\n",
    "The `ResponseVerificationAgent` checks whether the generated response is entirely supported by the retrieved excerpts. It validates the integrity of the response and ensures no unsupported information is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_verification_agent = ResponseVerificationAgent(llm)\n",
    "response_verification_task = Task(\n",
    "    description=\"Verify if the generated response is based solely on the retrieved document excerpts.\",\n",
    "    expected_output=\"Verification result indicating whether the response is supported by the excerpts.\",\n",
    "    agent=response_verification_agent, context=[llm_task]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6. Define and Execute the Agent Workflow\n",
    "The Crew framework is used to orchestrate the agent workflow, ensuring a sequential flow where each agent performs its task in a defined order. The context flow ensures that tasks pass information correctly, from query refinement to retrieval, response generation, and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "    agents=[query_refinement_agent, retrieval_agent, llm_agent, response_verification_agent],\n",
    "    tasks=[query_refinement_task, retrieval_task, llm_task, response_verification_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential,\n",
    "    context_flow={\n",
    "        retrieval_task: query_refinement_task,\n",
    "        llm_task: retrieval_task,\n",
    "        response_verification_task: llm_task  \n",
    "    }\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "print(result.raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.7 Output of the LLMAgent Task: Displaying Task Details and Outputs**\n",
    "\n",
    "After executing the LLMAgent task, the following output is retrieved and displayed. This includes the task's description, summary, raw output, and structured JSON output. The results are printed for review, providing a detailed breakdown of the generated content. If available, the JSON dictionary and Pydantic output are also printed for further inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_output = llm_task.output\n",
    "print(f\"Task Description: {task_output.description}\")\n",
    "print(f\"Task Summary: {task_output.summary}\")\n",
    "print(f\"Raw Output: {task_output.raw}\")\n",
    "print(f\"JSON Output: {json.loads(task_output.raw)}\")\n",
    "if task_output.json_dict:\n",
    "    print(f\"JSON Output: {json.dumps(task_output.json_dict, indent=2)}\")\n",
    "if task_output.pydantic:\n",
    "    print(f\"Pydantic Output: {task_output.pydantic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.8. Extract and Remove `<think>` Content for cleaner output**\n",
    "\n",
    "In this section, two functions are implemented to handle `<think>` tags within text:\n",
    "\n",
    "1. **`extract_think_content`**: This function extracts the content between the `<think>` and `</think>` tags. If such tags are found, it returns the content inside; otherwise, it returns an empty string.\n",
    "\n",
    "2. **`remove_think_content`**: This function removes all content enclosed within the `<think>` and `</think>` tags, ensuring that the text is cleaned of any such sections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_think_content(text):\n",
    "    match = re.search(r'<think>(.*?)</think>', text, flags=re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "# Function to remove everything between <think> and </think> tags\n",
    "def remove_think_content(text):\n",
    "    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Final response to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response=remove_think_content(json.loads(task_output.raw)['generated_response'])\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Reasoning for final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_final_response=extract_think_content(json.loads(task_output.raw)['generated_response'])\n",
    "print(reasoning_final_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Document Excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_documents = json.loads(task_output.raw)['retrieved_text']\n",
    "print(source_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Verify Response against Document Exceprts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_answer=remove_think_content(response_verification_task.output.raw)\n",
    "print(verify_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary**  \n",
    "\n",
    "This notebook implements a **Multi-Agentic RAG (Retrieval-Augmented Generation) workflow** using **CrewAI** to process documents, retrieve relevant excerpts, generate responses, and verify their accuracy.  \n",
    "\n",
    "- **Document Processing & Retrieval**: Documents (PDF, TXT, DOCX, HTML) are processed and embedded using **FAISS** for vector search.  \n",
    "- **Agents Workflow**: Four agents handle different tasks:  \n",
    "  1. **QueryRefinementAgent**: Enhances user queries for better search accuracy.  \n",
    "  2. **RetrievalAgent**: Fetches the most relevant document excerpts.  \n",
    "  3. **LLMAgent**: Generates a response strictly based on retrieved excerpts.  \n",
    "  4. **ResponseVerificationAgent**: Ensures the response is grounded in the retrieved content.  \n",
    "- **Execution Flow**: The agents are orchestrated sequentially via **CrewAI**, maintaining context across tasks.  \n",
    "- Enables **efficient document-based Q&A** with local LLM execution using **Ollama** for full control over inference.\n",
    "- **Final Output Processing**: The generated response is cleaned, extracting the final response, reasoning, and source documents.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
